from game import MDPGame, medium_level
from dynamic_programming import value_iteration, policy_iteration
from monte_carlo import mc_epsilon_greedy
from tuning import TuningParameters, test_hyperparameters, extract_hyperparameters, plot_hyperparameters
from q_learning import q_learning
from sarsa import sarsa
from utils import print_V, print_policy, test_policy, Q_to_policy
import sys

if __name__ == "__main__":
    args = sys.argv

    n = 1
    env = MDPGame(random_x=True)
    env.load_level(medium_level)

    if "-n" in args:
        n = int(args[args.index("-n")])

    print(args)

    # # Dynamic Programming
    # policy, V = value_iteration(env, gamma=0.95, theta=1e-10)
    # print("policy generated by value iteration: ")
    # test_policy(policy, env)
    # print_policy(policy, env)

    # policy, V = policy_iteration(env, gamma=0.95, theta=1e-10)
    # print("policy generated by value iteration: ")
    # test_policy(policy, env)
    # print_policy(policy, env)

    # # monte carlo
    # print(f"Testing montecarlo methods {n} times: ")
    # df = test_hyperparameters(env, n, TuningParameters.mc_epsilon_greedy, mc_epsilon_greedy)
    # plot_hyperparameters(df, "monte_carlo")
    # ideal_params = extract_hyperparameters(df)
    # print(f"ideal hyperparameters found {ideal_params}")
    # print("policy generated by montecarlo methods, using ideal parameters: ")

    # params = {**ideal_params, "env": env}
    # params.pop("pass_rate") # not an input parameter
    # Q = mc_epsilon_greedy(**params)
    # policy = Q_to_policy(Q, env)
    # test_policy(policy, env)
    # print_policy(policy, env)

    # # sarsa
    # print(f"Testing SARSA {n} times: ")
    # df = test_hyperparameters(env, n, TuningParameters.sarsa, sarsa)
    # plot_hyperparameters(df, "sarsa")
    # ideal_params = extract_hyperparameters(df)
    # print(f"ideal hyperparameters found {ideal_params}")
    # print("policy generated by SARSA, using ideal parameters: ")

    # params = {**ideal_params, "env": env}
    # params.pop("pass_rate") # not an input parameter
    # Q = sarsa(**params)
    # policy = Q_to_policy(Q, env)
    # test_policy(policy, env)
    # print_policy(policy, env)

    # # q-learning
    # print(f"Testing Q-learning {n} times: ")
    # df = test_hyperparameters(env, n, TuningParameters.q_learning, q_learning)
    # plot_hyperparameters(df, "sarsa")
    # ideal_params = extract_hyperparameters(df)
    # print(f"ideal hyperparameters found {ideal_params}")
    # print("policy generated by SARSA, using ideal parameters: ")

    # params = {**ideal_params, "env": env}
    # params.pop("pass_rate") # not an input parameter
    # Q = q_learning(**params)
    # policy = Q_to_policy(Q, env)
    # test_policy(policy, env)
    # print_policy(policy, env)
        
        

        




# if __name__ == "__main__":
#     env = MDPGame()
#     # env.load_level(hard_level)
#     # env.load_level(long_level)

#     print("-----Welcome to Reinforcement Learning!-----")
#     print("-----Select which algorithm you would like to inspect for our agent playing Surfing in the Subway:-----")
#     option = input("Policy Iteration (1) \n"
#                    "Value Iteration (2)\n"
#                    "Monte Carlo Control Epsilon Greedy (3)\n"
#                    "SARSA (4)\n"
#                    "Q-Learning (5)\n"
#                    "Stop playing (e) \n")

#     if option == "1":
#         optimal_policy, optimal_V = pi.policy_iteration(env)
#     elif option == "2":
#         optimal_policy, optimal_V = pi.value_iteration(env)
#     elif option == "3":
#         Q, optimal_policy = mc_epsilon_greedy(env, episodes=50000, epsilon=0.4, gamma=0.9)

#         pi.simulate_agent(env, optimal_policy)
#         optimal_V = Q_to_V(Q, env)
#         print_V(optimal_V, env)

#     elif option == "4":
#         Q = sarsa(env, episodes = 50000, alpha = 0.1, gamma = 0.9, epsilon = 0.1)
#         optimal_policy = Q_to_policy(Q, env)

#         optimal_V = {state: np.max(list(action_values.values())) for state, action_values in Q.items()}

#         print_policy(optimal_policy, env)
#         success_rate = test_policy(optimal_policy, env, count_partial_success=True)
#         print(f"SARSA success rate: {success_rate:.2f}")
#     elif option == "5":
#         Q = q_learning(env, episodes = 50000, alpha = 0.1, gamma = 0.9, epsilon = 0.1)
#         optimal_policy = Q_to_policy(Q, env)

#         optimal_V = {state: np.max(list(action_values.values())) for state, action_values in Q.items()}

#         print_policy(optimal_policy, env)
#         success_rate = test_policy(optimal_policy, env, count_partial_success=True)
#         print(f"Q-learning success rate: {success_rate:.2f}")

#     # Display the learned policy
#     for state in sorted(optimal_policy.keys()):
#         action = np.argmax(optimal_policy[state])  # Best action for each state
#         print(f"State {state}: Best Action -> {env.actions[action]}")

#     pi.inspect_policy(optimal_policy, env)
#     pi.inspect_value_function(optimal_V)
#     pi.print_policy_grid(optimal_policy, env)